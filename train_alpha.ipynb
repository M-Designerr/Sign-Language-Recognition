{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Keras libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense , Dropout\n",
    "import os\n",
    "import PIL\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"GPU-47357fb5-69ce-8a99-7c6f-1c7361564861\"\n",
    "sz = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# First convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape=(sz, sz, 1), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "classifier.add(Dense(units=128, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=96, activation='relu'))\n",
    "classifier.add(Dropout(0.40))\n",
    "classifier.add(Dense(units=64, activation='relu'))\n",
    "classifier.add(Dense(units=27, activation='softmax')) # softmax for more than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 126, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 63, 63, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 61, 61, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 30, 30, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 28800)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               3686528   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 96)                12384     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 11)                715       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,715,403\n",
      "Trainable params: 3,715,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']) # categorical_crossentropy for more than 2\n",
    "\n",
    "classifier.summary() # print the model summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Preparing the train/test data and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3724 files belonging to 11 classes.\n",
      "Using 2980 files for training.\n",
      "Found 3724 files belonging to 11 classes.\n",
      "Using 744 files for validation.\n",
      "Found 1235 files belonging to 11 classes.\n",
      "Epoch 1/5\n",
      "298/298 [==============================] - 33s 107ms/step - loss: 1.1942 - accuracy: 0.6295 - val_loss: 0.1325 - val_accuracy: 0.9960\n",
      "Epoch 2/5\n",
      "298/298 [==============================] - 31s 105ms/step - loss: 0.1826 - accuracy: 0.9530 - val_loss: 0.0067 - val_accuracy: 0.9987\n",
      "Epoch 3/5\n",
      "298/298 [==============================] - 31s 105ms/step - loss: 0.0899 - accuracy: 0.9698 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
      "Epoch 4/5\n",
      "298/298 [==============================] - 31s 105ms/step - loss: 0.0616 - accuracy: 0.9815 - val_loss: 3.9435e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "298/298 [==============================] - 32s 106ms/step - loss: 0.0496 - accuracy: 0.9846 - val_loss: 8.4849e-05 - val_accuracy: 1.0000\n",
      "124/124 [==============================] - 4s 28ms/step - loss: 0.2697 - accuracy: 0.9547\n",
      "Test accuracy: 0.9546558856964111\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the path to the data directory and the image size\n",
    "data_dir = 'data_alpha_1'\n",
    "img_size = (sz, sz)\n",
    "\n",
    "# Define the batch size and number of classes\n",
    "batch_size = 10\n",
    "num_classes = 2\n",
    "\n",
    "# Define the preprocessing layers\n",
    "preprocessing_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(scale=1./255),\n",
    "])\n",
    "\n",
    "# Load the training data\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(data_dir, 'train'),\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    image_size=img_size,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation='bilinear',\n",
    "    follow_links=True\n",
    ")\n",
    "\n",
    "# Apply the preprocessing layers to the training dataset\n",
    "train_ds = train_ds.map(lambda x, y: (preprocessing_layers(x), y))\n",
    "\n",
    "# Load the validation data\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(data_dir, 'train'),\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    image_size=img_size,\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation='bilinear',\n",
    "    follow_links=True\n",
    ")\n",
    "\n",
    "# Apply the preprocessing layers to the validation dataset\n",
    "val_ds = val_ds.map(lambda x, y: (preprocessing_layers(x), y))\n",
    "\n",
    "# Load the test data\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(data_dir, 'test'),\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    image_size=img_size,\n",
    "    shuffle=False,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=True\n",
    ")\n",
    "\n",
    "# Apply the preprocessing layers to the test dataset\n",
    "test_ds = test_ds.map(lambda x, y: (preprocessing_layers(x), y))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "classifier.fit(train_ds, epochs=5, validation_data=val_ds)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = classifier.evaluate(test_ds)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Weights saved\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "model_json = classifier.to_json()\n",
    "with open(\"model-bw-alpha_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print('Model Saved')\n",
    "classifier.save_weights('model-bw-alpha_1.h5')\n",
    "print('Weights saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
